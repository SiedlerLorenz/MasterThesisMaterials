{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ff09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b857e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession module\n",
    "from pyspark.sql import SparkSession\n",
    "# Import the SQL Context from PySpark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "import pyspark.sql.functions as F\n",
    "from operator import add\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ccee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[*]\") \\\n",
    "   .appName(\"PySpark_Test_Skript\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Get the SQL Context with the SparkContext Parameter\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = ['Data_loading_csv','Data_loading_json', 'Data_loading_parquet', 'Count_per_column', 'Mean_per_column',\n",
    "        'Sum_per_column', 'Standard_deviation', 'Summary', 'Filter', 'Avg_addition_2_columns', \n",
    "        'Sum_addition_2_columns', 'Product_2_columns', 'Add_new_column', 'Add_new_column_complex_calculation', \n",
    "        'GroupBy', 'Distinct', 'Join']\n",
    "\n",
    "dataframe_dimensions = ['1/1', '1/10', '1/100', '1/1000', '1/10000', '1/100000', '1/1000000', '1/10000000',\n",
    "                   '5/1', '5/10', '5/100', '5/1000', '5/10000', '5/100000', '5/1000000', '5/10000000',\n",
    "                   '10/1', '10/10', '10/100', '10/1000', '10/10000', '10/100000', '10/1000000', '10/10000000',\n",
    "                   '20/1', '20/10', '20/100', '20/1000', '20/10000', '20/100000', '20/1000000', '20/10000000',\n",
    "                   '30/1', '30/10', '30/100', '30/1000', '30/10000', '30/100000', '30/1000000', '30/10000000',\n",
    "                   '40/1', '40/10', '40/100', '40/1000', '40/10000', '40/100000', '40/1000000', '40/10000000',]\n",
    "\n",
    "time_df = pd.DataFrame({'Data_loading_csv':pd.Series(dtype='float'),\n",
    "                        'Data_loading_json':pd.Series(dtype='float'),\n",
    "                        'Data_loading_parquet':pd.Series(dtype='float'),\n",
    "                        'Count_per_column':pd.Series(dtype='float'),\n",
    "                        'Mean_per_column':pd.Series(dtype='float'),\n",
    "                        'Median_per_column':pd.Series(dtype='float'),\n",
    "                        'Max_per_column':pd.Series(dtype='float'),\n",
    "                        'Min_per_column':pd.Series(dtype='float'),\n",
    "                        'Sum_per_column':pd.Series(dtype='float'),\n",
    "                        'Standard_deviation_per_column':pd.Series(dtype='float'),\n",
    "                        'Summary':pd.Series(dtype='float'),\n",
    "                        'Filter':pd.Series(dtype='float'),\n",
    "                        'Avg_addition_2_columns':pd.Series(dtype='float'),\n",
    "                        'Sum_addition_2_columns':pd.Series(dtype='float'),\n",
    "                        'Product_addition_2_columns':pd.Series(dtype='float'),\n",
    "                        'Add_new_column':pd.Series(dtype='float'),\n",
    "                        'Add_new_column_comparing_size':pd.Series(dtype='float'),\n",
    "                        'GroupBy':pd.Series(dtype='float'),\n",
    "                        'Distinct':pd.Series(dtype='float'),\n",
    "                        'Number_distinct_values':pd.Series(dtype='float'),\n",
    "                        'Join_raw':pd.Series(dtype='float'),\n",
    "                        'Join':pd.Series(dtype='float'),\n",
    "                        }, index=dataframe_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './Dataframes/'\n",
    "\n",
    "df_support = spark.read.csv(\"Support_Dataframe_6_1000.csv\", header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de08a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_measurement(function, df, function_name, dataframe_dimension, nr_repetitions=1, **kwargs):\n",
    "    measured_times = []\n",
    "    for i in range(nr_repetitions):\n",
    "        start_time = time.time()\n",
    "        ret = function(df, **kwargs)\n",
    "        measured_times.append(time.time()-start_time)\n",
    "    time_df.loc[dataframe_dimension, function_name] = np.mean(measured_times)\n",
    "    print(f\"{function_name} - {dataframe_dimension} - Time: {time_df.loc[dataframe_dimension, function_name]}\")\n",
    "    return ret\n",
    "    \n",
    "def Data_loading_csv (df, file_name):\n",
    "    return spark.read.csv(PATH + file_name, header=True) \n",
    "    \n",
    "def Data_loading_parquet(df, file_name):\n",
    "    return spark.read.parquet(PATH + file_name)\n",
    "\n",
    "def Data_loading_json(df, file_name):\n",
    "    return spark.read.json(PATH + file_name)#, multiLine=True)\n",
    "\n",
    "def Count_per_column(df):\n",
    "    return df.select([F.count(F.when(F.col(c).isNotNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "def Mean_per_column(df):\n",
    "    return df.select([F.mean(c).alias(c) for c in df.columns])\n",
    "\n",
    "def Median_per_column(df):\n",
    "    return df.select([F.percentile_approx(c, 0.5).alias(c) for c in df.columns]) #PC, Cluster\n",
    "    #return [df.approxQuantile(c, [0.5], 0.01) for c in df.columns] #Server\n",
    "\n",
    "def Max_per_column(df):\n",
    "    return df.select([F.max(c).alias(c) for c in df.columns])\n",
    "\n",
    "def Min_per_column(df):\n",
    "    return df.select([F.min(c).alias(c) for c in df.columns])\n",
    "\n",
    "def Sum_per_column(df):\n",
    "    return df.select([F.sum(c).alias(c) for c in df.columns])\n",
    "\n",
    "def Standard_deviation_per_column(df):\n",
    "    return df.select([F.stddev(c).alias(c) for c in df.columns])\n",
    "\n",
    "def Summary(df):\n",
    "    return df.summary()\n",
    "\n",
    "def Filter(df, column, upperbound, lowerbound):\n",
    "    return df.filter((F.col(column) > lowerbound)&(F.col(column) < upperbound))\n",
    "\n",
    "def Avg_addition_2_columns(df, column_1, column_2):\n",
    "    return df.select(((F.col(column_1) + F.col(column_2))/2))#.alias(\"Avg_addition_2_columns\"))\n",
    "\n",
    "def Sum_addition_2_columns(df, column_1, column_2):\n",
    "    return df.select((F.col(column_1) + F.col(column_2)))#.alias(\"Sum_addition_2_columns\"))\n",
    "\n",
    "def Product_addition_2_columns(df, column_1, column_2):\n",
    "    return df.select((F.col(column_1) * F.col(column_2)))#.alias(\"Product_addition_2_columns\"))\n",
    "\n",
    "def Add_new_column(df, df_2, column_1, column_2, column_name): \n",
    "    df_2 = df_2.withColumn(column_name, F.col(column_1) + F.col(column_2))\n",
    "    return df_2\n",
    "\n",
    "def Add_new_column_comparing_size(df, df_2, column_1, column_2, column_name):\n",
    "    df_2 = df_2.withColumn(column_name, F.when((F.col(column_1) > F.col(column_2)), 1).when((F.col(column_1) < F.col(column_2)), 2).otherwise(\"Tie\"))\n",
    "    return df_2\n",
    "\n",
    "def GroupBy(df, column):\n",
    "    return df.groupBy(column).agg(*[F.sum(c).alias(c) for c in df.columns if c != column])\n",
    "\n",
    "def Distinct(df, column):\n",
    "    return df.select(column).distinct()\n",
    "\n",
    "def Number_distinct_values(df):\n",
    "    return df.select([F.countDistinct(c).alias(c) for c in df.columns])\n",
    "\n",
    "def Join_raw(df_1, df_2):\n",
    "    return df_1.join(df_2, df_1.id == df_2.id, \"left\").drop(df_2.id)\n",
    "\n",
    "def Join(df_1, df_2, column):\n",
    "    df_supp_1 = df_1.groupBy(column).agg(*[F.sum(c).alias(c) for c in df_1.columns if c != column])\n",
    "    df_supp_2 = df_2.groupBy(column).agg(*[F.sum(c).alias(c) for c in df_2.columns if c != column])\n",
    "    return df_supp_1.join(df_supp_2, df_supp_1.id == df_supp_2.id, \"left\").drop(df_supp_2.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataframe_dimensions:\n",
    "    x,y = i.split('/')\n",
    "    \n",
    "    time_measurement(Data_loading_csv, None, 'Data_loading_csv', i, 1, file_name=f\"Dataframe_{x}_{y}.csv\")\n",
    "    df = time_measurement(Data_loading_parquet, None, 'Data_loading_parquet', i, 1, file_name=f\"Dataframe_{x}_{y}.parquet.gz\")\n",
    "    time_measurement(Data_loading_json, None, 'Data_loading_json', i, 1, file_name=f\"Dataframe_{x}_{y}.json.gz\")\n",
    "\n",
    "    df_2 = df\n",
    "\n",
    "    time_measurement(Count_per_column, df, 'Count_per_column', i)\n",
    "    time_measurement(Mean_per_column, df, 'Mean_per_column', i)\n",
    "    time_measurement(Median_per_column, df, 'Median_per_column', i)\n",
    "    time_measurement(Max_per_column, df, 'Max_per_column', i)\n",
    "    time_measurement(Min_per_column, df, 'Min_per_column', i)\n",
    "    time_measurement(Sum_per_column, df, 'Sum_per_column', i)\n",
    "    \n",
    "    time_measurement(Standard_deviation_per_column, df, 'Standard_deviation_per_column', i)\n",
    "    time_measurement(Summary, df, 'Summary', i)\n",
    "    time_measurement(Filter, df, 'Filter', i, column='col0', upperbound=80, lowerbound=-40)\n",
    "    \n",
    "    if x == '1':\n",
    "        time_measurement(Avg_addition_2_columns, df, 'Avg_addition_2_columns', i, column_1='col0', column_2='col0')\n",
    "        time_measurement(Sum_addition_2_columns, df, 'Sum_addition_2_columns', i, column_1='col0', column_2='col0')\n",
    "\n",
    "        time_measurement(Product_addition_2_columns, df, 'Product_addition_2_columns', i, column_1='col0', column_2='col0')\n",
    "        time_measurement(Add_new_column, df, 'Add_new_column', i, df_2=df_2, column_1='col0', column_2='col0', column_name = 'Add_col')\n",
    "        time_measurement(Add_new_column_comparing_size, df, 'Add_new_column_comparing_size', i, df_2=df_2, column_1='col0', column_2='col0', column_name = 'Comp_Size')\n",
    "        time_measurement(GroupBy, df, 'GroupBy', i, column='id')\n",
    "        time_measurement(Distinct, df, 'Distinct', i, column='col0')\n",
    "    else:   \n",
    "        time_measurement(Avg_addition_2_columns, df, 'Avg_addition_2_columns', i, column_1='col2', column_2='col3')\n",
    "        time_measurement(Sum_addition_2_columns, df, 'Sum_addition_2_columns', i, column_1='col2', column_2='col3')\n",
    "\n",
    "        time_measurement(Product_addition_2_columns, df, 'Product_addition_2_columns', i, column_1='col2', column_2='col3')\n",
    "        time_measurement(Add_new_column, df, 'Add_new_column', i, df_2=df_2, column_1='col2', column_2='col3', column_name = 'Add_col')\n",
    "        time_measurement(Add_new_column_comparing_size, df, 'Add_new_column_comparing_size', i, df_2=df_2, column_1='col2', column_2='col3', column_name = 'Comp_Size')\n",
    "        time_measurement(GroupBy, df, 'GroupBy', i, column='id')\n",
    "        time_measurement(Distinct, df, 'Distinct', i, column='col2')\n",
    "        \n",
    "    time_measurement(Number_distinct_values, df, 'Number_distinct_values', i)\n",
    "    time_measurement(Join_raw, df, 'Join_raw', i, df_2=df_support)\n",
    "    time_measurement(Join, df, 'Join', i, df_2=df_support, column='id')\n",
    "\n",
    "display(time_df)\n",
    "time_df.to_csv(f\"./Results/PC_PySpark_Result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b5acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
